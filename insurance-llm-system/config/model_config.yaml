# LLM Configuration
llm:
  openai:
    api_key: "your_openai_key"
    model: "gpt-4-1106-preview"  # or "gpt-3.5-turbo"
    embedding_model: "text-embedding-ada-002"
    temperature: 0.7
    max_tokens: 1000
    timeout: 30
  
  local:
    use_local: false
    model_name: "meta-llama/Llama-2-7b-chat-hf"
    cache_dir: "./model_cache"
    temperature: 0.7
    max_tokens: 1000
    fallback_to_openai: true

embedding:
  use_local: false
  sentence_transformer_model: "all-MiniLM-L6-v2"
  batch_size: 32

training:
  base_model: "bert-base-uncased"
  epochs: 3
  batch_size: 8
  eval_batch_size: 16
  learning_rate: 2e-5
  output_dir: "./output_models"
  eval_metrics: ["accuracy", "f1", "precision", "recall"]
  eval_split_ratio: 0.2