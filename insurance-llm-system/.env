# ======================
# Application Core Settings
# ======================
APP_ENV=development  # or "production"
DEBUG=True
API_KEY=your_secret_api_key  # For securing API endpoints
CORS_ORIGINS=*  # Comma-separated list of allowed origins

# ======================
# Database Configuration
# ======================
DB_URL=postgresql://user:password@localhost:5432/insurance_db
TEST_DB_URL=postgresql://user:password@localhost:5432/insurance_test_db

# ======================
# Document Processing
# ======================
# PDF Processing
PDF_MAX_PAGES=100  # Maximum pages to process from PDFs
PDF_TEXT_EXTRACTION_MODE=accurate  # "fast" or "accurate"

# Text Processing
CHUNK_SIZE=1000  # Characters per chunk
CHUNK_OVERLAP=200  # Overlap between chunks
MAX_DOCUMENT_SIZE_MB=10  # Max file size to process
ALLOWED_FILE_TYPES=pdf,docx,txt,eml  # Comma-separated list

# Text Cleaning
CLEANER_LOWERCASE=True
CLEANER_REMOVE_SPECIAL_CHARS=True
CLEANER_REMOVE_STOPWORDS=False
CLEANER_STOPWORDS=the,and,or  # Comma-separated stopwords

# ======================
# Vector Database
# ======================
VECTOR_DB=pinecone  # "pinecone" or "chroma"

# Pinecone Settings
PINECONE_API_KEY=your_pinecone_key
PINECONE_ENVIRONMENT=us-west1-gcp
PINECONE_INDEX_NAME=insurance-docs

# ChromaDB Settings
CHROMA_COLLECTION_NAME=insurance-clauses
CHROMA_PERSIST_DIR=./chroma_db  # Where to persist ChromaDB data

# ======================
# LLM Configuration
# ======================
# OpenAI Settings
OPENAI_API_KEY=your_openai_key
OPENAI_MODEL=gpt-4-1106-preview  # or "gpt-3.5-turbo"
OPENAI_EMBEDDING_MODEL=text-embedding-ada-002

# Local LLM Settings (Llama 2)
USE_LOCAL_LLM=False  # Set to True to use local model
LLAMA_MODEL_NAME=meta-llama/Llama-2-7b-chat-hf
MODEL_CACHE_DIR=./model_cache  # Where to cache downloaded models
LLAMA_TEMPERATURE=0.7
LLAMA_MAX_TOKENS=1000

# Model Selection
FALLBACK_TO_OPENAI=True  # Fallback to OpenAI if local fails

# ======================
# Embedding Models
# ======================
USE_LOCAL_EMBEDDINGS=False  # Set to True to use local embeddings
SENTENCE_TRANSFORMER_MODEL=all-MiniLM-L6-v2  # Local embedding model
EMBEDDING_BATCH_SIZE=32  # For batch processing

# ======================
# Training Configuration
# ======================
# Fine-tuning
FINE_TUNING_BASE_MODEL=bert-base-uncased
FINE_TUNING_EPOCHS=3
TRAIN_BATCH_SIZE=8
EVAL_BATCH_SIZE=16
MODEL_OUTPUT_DIR=./output_models
LEARNING_RATE=2e-5

# Evaluation
EVAL_METRICS=accuracy,f1,precision,recall
EVAL_SPLIT_RATIO=0.2  # 20% for evaluation

# ======================
# Performance Tuning
# ======================
MAX_CONCURRENT_PROCESSES=4  # For document processing
EMBEDDING_PARALLELISM=2  # For parallel embedding generation
LLM_TIMEOUT_SECONDS=30  # Timeout for LLM requests
DB_POOL_SIZE=20  # Database connection pool size
DB_MAX_OVERFLOW=30

# ======================
# Logging Configuration
# ======================
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR
LOG_FILE=./logs/app.log
LOG_MAX_SIZE_MB=10  # Max log file size
LOG_BACKUP_COUNT=5  # Number of backup logs to keep